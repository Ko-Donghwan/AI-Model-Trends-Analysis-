{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceModelDownloader:\n",
    "    def __init__(self, csv_file, output_file, max_retries=5, delay=5):\n",
    "        self.csv_file = csv_file\n",
    "        self.output_file = output_file\n",
    "        self.max_retries = max_retries\n",
    "        self.delay = delay\n",
    "        self.model_list = self._load_model_list()\n",
    "\n",
    "    def _load_model_list(self):\n",
    "        df = pd.read_csv(self.csv_file)\n",
    "        result_df = df.sort_values(by='total_score', ascending=False)\n",
    "        model_list = result_df['id'].tolist()\n",
    "        return model_list\n",
    "\n",
    "    def _download_model_page(self, model_name):\n",
    "        url = f'https://huggingface.co/{model_name}'\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, timeout=10)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                return soup\n",
    "            except (requests.RequestException, requests.Timeout) as e:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.delay)\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "    def _extract_data(self, soup):\n",
    "        task_doc = soup.select_one('body > div > main > div.SVELTE_HYDRATER.contents > header > div > div.mb-3.flex.flex-wrap.md\\:mb-4')\n",
    "        task = (None if task_doc is None else (task_doc.find('a', href=re.compile(r'/models\\?pipeline_tag=.*')).text.strip() if task_doc.find('a', href=re.compile(r'/models\\?pipeline_tag=.*')) else None))    \n",
    "\n",
    "        downloads = soup.select_one('body > div:nth-of-type(1) > main > div:nth-of-type(2) > section:nth-of-type(2) > div:nth-of-type(1) > dl > dd')\n",
    "        total = None if downloads is None else downloads.text.replace(',', '')\n",
    "\n",
    "        target_div = soup.select_one('html > body > div > main > div:nth-of-type(2) > section:nth-of-type(2) > div:nth-of-type(1) > div')\n",
    "        d_attribute = (target_div := soup.select_one('html > body > div > main > div:nth-of-type(2) > section:nth-of-type(2) > div:nth-of-type(1) > div')) and (first_path := target_div.find('path')) and first_path.get('d', None) or None\n",
    "\n",
    "        return task, total, d_attribute\n",
    "\n",
    "    def _parse_path_data(self, path_data):\n",
    "        coordinates = re.findall(r'[-+]?[0-9]*\\.?[0-9]+', path_data)\n",
    "        points = [(float(coordinates[i]), float(coordinates[i + 1])) \n",
    "                  for i in range(0, len(coordinates), 2)]\n",
    "        return points\n",
    "\n",
    "    def _calculate_graph(self, path_data, download_total):\n",
    "        if path_data is None:\n",
    "            return [0] * 30\n",
    "        else:\n",
    "            try:\n",
    "                points = self._parse_path_data(path_data)\n",
    "            except IndexError:\n",
    "                return [0] * 30\n",
    "\n",
    "            max_day = max(point[0] for point in points)\n",
    "            scale_factor = 30 / max_day\n",
    "            normalized_points = [(x * scale_factor, 100 - y) for x, y in points]\n",
    "            daily_downloads = [0] * 30\n",
    "\n",
    "            for i in range(len(normalized_points) - 1):\n",
    "                start_day, start_value = normalized_points[i]\n",
    "                end_day, end_value = normalized_points[i + 1]\n",
    "                start_day_int = int(start_day)\n",
    "                end_day_int = int(end_day)\n",
    "                range_span = end_day - start_day\n",
    "\n",
    "                if start_day_int == end_day_int:\n",
    "                    if start_day_int < 30:\n",
    "                        daily_downloads[start_day_int] += (start_value + end_value) / 2\n",
    "                else:\n",
    "                    increment = (end_value - start_value) / range_span\n",
    "                    for j in range(start_day_int, min(end_day_int + 1, 30)):\n",
    "                        if j == start_day_int:\n",
    "                            fraction = 1 - (start_day - start_day_int)\n",
    "                            daily_downloads[j] += start_value + fraction * increment\n",
    "                        elif j == end_day_int:\n",
    "                            fraction = end_day - end_day_int\n",
    "                            daily_downloads[j] += start_value + fraction * increment\n",
    "                        else:\n",
    "                            fraction = (j - start_day) / (end_day - start_day)\n",
    "                            daily_downloads[j] += start_value + fraction * increment\n",
    "\n",
    "            total_downloads = sum(daily_downloads)\n",
    "            scaling_factor = float(download_total) / total_downloads\n",
    "            daily_downloads = [d * scaling_factor for d in daily_downloads]\n",
    "            daily_downloads = [int(round(d)) for d in daily_downloads]\n",
    "\n",
    "            difference = int(download_total) - sum(daily_downloads)\n",
    "            adjustment_indices = list(range(30)) if difference > 0 else list(range(29, -1, -1))\n",
    "\n",
    "            for i in range(abs(difference)):\n",
    "                daily_downloads[adjustment_indices[i % 30]] += 1 if difference > 0 else -1\n",
    "\n",
    "            for i in range(30):\n",
    "                if daily_downloads[i] < 0:\n",
    "                    surplus = -daily_downloads[i]\n",
    "                    daily_downloads[i] = 0\n",
    "                    for j in range(30):\n",
    "                        if daily_downloads[j] > surplus:\n",
    "                            daily_downloads[j] -= surplus\n",
    "                            break\n",
    "                        else:\n",
    "                            surplus -= daily_downloads[j]\n",
    "                            daily_downloads[j] = 0\n",
    "\n",
    "            return daily_downloads\n",
    "\n",
    "    def save_model_data(self, model_name):\n",
    "        soup = self._download_model_page(model_name)\n",
    "        model_task, download_total, path_data = self._extract_data(soup)\n",
    "        daily_downloads = self._calculate_graph(path_data, download_total)\n",
    "        if model_task and model_task.lower() == 'transformers':\n",
    "            model_task = None\n",
    "            \n",
    "        data = {\n",
    "            \"Model_name\": [model_name],\n",
    "            \"Model_task\": [model_task],\n",
    "            **{f\"{i+1}Day\": [daily_downloads[i]] for i in range(30)},\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        if os.path.exists(self.output_file):\n",
    "            existing_df = pd.read_csv(self.output_file)\n",
    "            combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "        else:\n",
    "            combined_df = df\n",
    "        \n",
    "        combined_df.to_csv(self.output_file, index=False)\n",
    "\n",
    "    def collect_data(self, start_index, end_index):\n",
    "        print(f\"총 행 수: {len(self.model_list)} 행\")\n",
    "        for i in range(start_index, end_index):\n",
    "            print(self.model_list[i])\n",
    "            self.save_model_data(self.model_list[i])\n",
    "            print(f\"{i+1}번 모델 데이터 수집 중입니다.\")\n",
    "        print(\"Data collection completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "downloader = HuggingFaceModelDownloader(\n",
    "    csv_file=\"./sorted_hugging_face_model_influence_with_scores_0615.csv\",\n",
    "    output_file=\"20240617_Daily_Download.csv\"\n",
    ")\n",
    "downloader.collect_data(0, 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
