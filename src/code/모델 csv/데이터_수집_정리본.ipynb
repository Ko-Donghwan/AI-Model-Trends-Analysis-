{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d13442-b16b-436f-95cd-f2a8ea6e63c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching model metadata: 717038it [06:48, 1756.69it/s]\n",
      "Processing tags: 100%|███████████████████████████████████████████████████████| 717038/717038 [02:04<00:00, 5739.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# Hugging Face API 인스턴스 생성\n",
    "api = HfApi()\n",
    "\n",
    "# 모델 리스트를 가져오기 (모델 메타 데이터 포함)\n",
    "models_generator = api.list_models(full=True)\n",
    "\n",
    "# 모델 메타 데이터를 저장할 리스트 초기화\n",
    "models_data = []\n",
    "\n",
    "# 모델 메타 데이터를 가져오는 동안 진행도 표시\n",
    "for model in tqdm(models_generator, desc=\"Fetching model metadata\"):\n",
    "    models_data.append(model)\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "df = pd.DataFrame(models_data)\n",
    "\n",
    "# 불필요한 컬럼 삭제\n",
    "columns_to_drop = ['siblings', 'sha', 'mask_token', 'card_data', 'widget_data', 'model_index', 'config', 'transformers_info', 'spaces', 'safetensors']\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# 2. 새로운 컬럼 생성\n",
    "df['arxiv'] = ''\n",
    "df['dataset'] = ''\n",
    "df['region'] = ''\n",
    "df['license'] = ''\n",
    "\n",
    "# 3. tags 컬럼의 객체들을 각 컬럼에 입력하고 tags에서 삭제\n",
    "def process_tags(tags):\n",
    "    arxiv, dataset, region, license = '', '', '', ''\n",
    "    new_tags = []\n",
    "    for tag in tags:\n",
    "        lower_tag = tag.lower()\n",
    "        if lower_tag.startswith('arxiv:'):\n",
    "            arxiv = tag\n",
    "        elif lower_tag.startswith('dataset:'):\n",
    "            dataset = tag\n",
    "        elif lower_tag.startswith('region:'):\n",
    "            region = tag\n",
    "        elif lower_tag.startswith('license:'):\n",
    "            license = tag\n",
    "        else:  # 특정 객체를 삭제하지 않음\n",
    "            new_tags.append(tag)\n",
    "    return arxiv, dataset, region, license, new_tags\n",
    "\n",
    "# 진행도를 표시하면서 tags 컬럼 처리\n",
    "for index in tqdm(range(len(df)), desc=\"Processing tags\"):\n",
    "    tags_data = df.at[index, 'tags']\n",
    "    if isinstance(tags_data, str):\n",
    "        tags_data = ast.literal_eval(tags_data)\n",
    "    arxiv, dataset, region, license, new_tags = process_tags(tags_data)\n",
    "    df.at[index, 'arxiv'] = arxiv\n",
    "    df.at[index, 'dataset'] = dataset\n",
    "    df.at[index, 'region'] = region\n",
    "    df.at[index, 'license'] = license\n",
    "    df.at[index, 'tags'] = new_tags\n",
    "\n",
    "# 4. 수정된 데이터 프레임을 CSV 파일로 저장한다.\n",
    "output_csv_file = '6-15.csv'\n",
    "df.to_csv(output_csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9f80a0-3c10-418a-a6c3-d6ba7151d13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>created_at</th>\n",
       "      <th>last_modified</th>\n",
       "      <th>private</th>\n",
       "      <th>gated</th>\n",
       "      <th>disabled</th>\n",
       "      <th>downloads</th>\n",
       "      <th>likes</th>\n",
       "      <th>library_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>pipeline_tag</th>\n",
       "      <th>arxiv</th>\n",
       "      <th>dataset</th>\n",
       "      <th>region</th>\n",
       "      <th>license</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert/albert-base-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:57:35+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>21912</td>\n",
       "      <td>7</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, safetensors, alber...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>arxiv:1909.11942</td>\n",
       "      <td>dataset:wikipedia</td>\n",
       "      <td>region:us</td>\n",
       "      <td>license:apache-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>albert/albert-base-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:58:14+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2509354</td>\n",
       "      <td>90</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, jax, rust, safeten...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>arxiv:1909.11942</td>\n",
       "      <td>dataset:wikipedia</td>\n",
       "      <td>region:us</td>\n",
       "      <td>license:apache-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>albert/albert-large-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:58:26+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>2184</td>\n",
       "      <td>3</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, albert, fill-mask,...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>arxiv:1909.11942</td>\n",
       "      <td>dataset:wikipedia</td>\n",
       "      <td>region:us</td>\n",
       "      <td>license:apache-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>albert/albert-large-v2</td>\n",
       "      <td>albert</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 10:58:48+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>68390</td>\n",
       "      <td>14</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, safetensors, alber...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>arxiv:1909.11942</td>\n",
       "      <td>dataset:wikipedia</td>\n",
       "      <td>region:us</td>\n",
       "      <td>license:apache-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>albert/albert-xlarge-v1</td>\n",
       "      <td>albert</td>\n",
       "      <td>2022-03-02 23:29:04+00:00</td>\n",
       "      <td>2024-02-19 11:01:28+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>1597</td>\n",
       "      <td>4</td>\n",
       "      <td>transformers</td>\n",
       "      <td>[transformers, pytorch, tf, safetensors, alber...</td>\n",
       "      <td>fill-mask</td>\n",
       "      <td>arxiv:1909.11942</td>\n",
       "      <td>dataset:wikipedia</td>\n",
       "      <td>region:us</td>\n",
       "      <td>license:apache-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665553</th>\n",
       "      <td>mazzaqq/SFT_Llama_Large</td>\n",
       "      <td>mazzaqq</td>\n",
       "      <td>2024-05-21 08:09:40+00:00</td>\n",
       "      <td>2024-05-21 08:09:41+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>region:us</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665554</th>\n",
       "      <td>viethq5/sft_llama3_v3</td>\n",
       "      <td>viethq5</td>\n",
       "      <td>2024-05-21 08:09:41+00:00</td>\n",
       "      <td>2024-05-21 08:09:43+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>region:us</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665555</th>\n",
       "      <td>KaranChand/phi-ft-100000-fp</td>\n",
       "      <td>KaranChand</td>\n",
       "      <td>2024-05-21 08:09:43+00:00</td>\n",
       "      <td>2024-05-21 08:09:43+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>region:us</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665556</th>\n",
       "      <td>AlignmentResearch/robust_llm_pythia-14m_pretra...</td>\n",
       "      <td>AlignmentResearch</td>\n",
       "      <td>2024-05-21 08:09:44+00:00</td>\n",
       "      <td>2024-05-21 08:09:44+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>region:us</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665557</th>\n",
       "      <td>AlignmentResearch/robust_llm_pythia-14m_pretra...</td>\n",
       "      <td>AlignmentResearch</td>\n",
       "      <td>2024-05-21 08:09:48+00:00</td>\n",
       "      <td>2024-05-21 08:09:48+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>region:us</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>665558 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       id             author  \\\n",
       "0                                   albert/albert-base-v1             albert   \n",
       "1                                   albert/albert-base-v2             albert   \n",
       "2                                  albert/albert-large-v1             albert   \n",
       "3                                  albert/albert-large-v2             albert   \n",
       "4                                 albert/albert-xlarge-v1             albert   \n",
       "...                                                   ...                ...   \n",
       "665553                            mazzaqq/SFT_Llama_Large            mazzaqq   \n",
       "665554                              viethq5/sft_llama3_v3            viethq5   \n",
       "665555                        KaranChand/phi-ft-100000-fp         KaranChand   \n",
       "665556  AlignmentResearch/robust_llm_pythia-14m_pretra...  AlignmentResearch   \n",
       "665557  AlignmentResearch/robust_llm_pythia-14m_pretra...  AlignmentResearch   \n",
       "\n",
       "                      created_at             last_modified  private  gated  \\\n",
       "0      2022-03-02 23:29:04+00:00 2024-02-19 10:57:35+00:00    False  False   \n",
       "1      2022-03-02 23:29:04+00:00 2024-02-19 10:58:14+00:00    False  False   \n",
       "2      2022-03-02 23:29:04+00:00 2024-02-19 10:58:26+00:00    False  False   \n",
       "3      2022-03-02 23:29:04+00:00 2024-02-19 10:58:48+00:00    False  False   \n",
       "4      2022-03-02 23:29:04+00:00 2024-02-19 11:01:28+00:00    False  False   \n",
       "...                          ...                       ...      ...    ...   \n",
       "665553 2024-05-21 08:09:40+00:00 2024-05-21 08:09:41+00:00    False  False   \n",
       "665554 2024-05-21 08:09:41+00:00 2024-05-21 08:09:43+00:00    False  False   \n",
       "665555 2024-05-21 08:09:43+00:00 2024-05-21 08:09:43+00:00    False  False   \n",
       "665556 2024-05-21 08:09:44+00:00 2024-05-21 08:09:44+00:00    False  False   \n",
       "665557 2024-05-21 08:09:48+00:00 2024-05-21 08:09:48+00:00    False  False   \n",
       "\n",
       "       disabled  downloads  likes  library_name  \\\n",
       "0          None      21912      7  transformers   \n",
       "1          None    2509354     90  transformers   \n",
       "2          None       2184      3  transformers   \n",
       "3          None      68390     14  transformers   \n",
       "4          None       1597      4  transformers   \n",
       "...         ...        ...    ...           ...   \n",
       "665553     None          0      0          None   \n",
       "665554     None          0      0          None   \n",
       "665555     None          0      0          None   \n",
       "665556     None          0      0          None   \n",
       "665557     None          0      0          None   \n",
       "\n",
       "                                                     tags pipeline_tag  \\\n",
       "0       [transformers, pytorch, tf, safetensors, alber...    fill-mask   \n",
       "1       [transformers, pytorch, tf, jax, rust, safeten...    fill-mask   \n",
       "2       [transformers, pytorch, tf, albert, fill-mask,...    fill-mask   \n",
       "3       [transformers, pytorch, tf, safetensors, alber...    fill-mask   \n",
       "4       [transformers, pytorch, tf, safetensors, alber...    fill-mask   \n",
       "...                                                   ...          ...   \n",
       "665553                                                 []         None   \n",
       "665554                                                 []         None   \n",
       "665555                                                 []         None   \n",
       "665556                                                 []         None   \n",
       "665557                                                 []         None   \n",
       "\n",
       "                   arxiv            dataset     region             license  \n",
       "0       arxiv:1909.11942  dataset:wikipedia  region:us  license:apache-2.0  \n",
       "1       arxiv:1909.11942  dataset:wikipedia  region:us  license:apache-2.0  \n",
       "2       arxiv:1909.11942  dataset:wikipedia  region:us  license:apache-2.0  \n",
       "3       arxiv:1909.11942  dataset:wikipedia  region:us  license:apache-2.0  \n",
       "4       arxiv:1909.11942  dataset:wikipedia  region:us  license:apache-2.0  \n",
       "...                  ...                ...        ...                 ...  \n",
       "665553                                       region:us                      \n",
       "665554                                       region:us                      \n",
       "665555                                       region:us                      \n",
       "665556                                       region:us                      \n",
       "665557                                       region:us                      \n",
       "\n",
       "[665558 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359fa385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Model_2025-02-11.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65869772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'author', 'sha', 'created_at', 'last_modified', 'private',\n",
       "       'disabled', 'downloads', 'downloads_all_time', 'gated', 'gguf',\n",
       "       'inference', 'likes', 'library_name', 'tags', 'pipeline_tag',\n",
       "       'trending_score', 'security_repo_status', 'arxiv', 'dataset', 'region',\n",
       "       'license'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4901a09a-61ca-4bf7-a031-2e837bb9e51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique libraries in the dataset:\n",
      "transformers\n",
      "diffusers\n",
      "vllm\n",
      "hunyuan3d-2.0\n",
      "lerobot\n",
      "ben2\n",
      "birefnet\n",
      "sentence-transformers\n",
      "hibiki\n",
      "coqui\n",
      "transformers.js\n",
      "liveportrait\n",
      "trellis\n",
      "chat_tts\n",
      "pyannote-audio\n",
      "gguf\n",
      "granite-tsfm\n",
      "f5-tts\n",
      "diffusion-single-file\n",
      "ultralytics\n",
      "sam2\n",
      "timesfm\n",
      "mlx\n",
      "ctranslate2\n",
      "stable-audio-tools\n",
      "hunyuan3d-1.0\n",
      "cosyvoice\n",
      "stable-diffusion\n",
      "colpali\n",
      "open-oasis\n",
      "peft\n",
      "moshi\n",
      "t5\n",
      "speechbrain\n",
      "open_clip\n",
      "seamless_communication\n",
      "PyTorch\n",
      "nemo\n",
      "cosmos\n",
      "fasttext\n",
      "gliner\n",
      "reverb\n",
      "coreml\n",
      "adapter-transformers\n",
      "tf-keras\n",
      "mlx-llm\n",
      "pytorch\n",
      "tflite\n",
      "hunyuan-dit\n",
      "timm\n",
      "Tevatron\n",
      "mini-omni2\n",
      "model2vec\n",
      "timer\n",
      "speakerkit\n",
      "bertopic\n",
      "dghs-imgutils\n",
      "sana\n",
      "swarmformer\n",
      "dghs-realutils\n",
      "llamacpp\n",
      "stable-baselines3\n",
      "keras\n",
      "ml-agents\n",
      "espnet\n",
      "fairseq\n",
      "flair\n",
      "pysentimiento\n",
      "spacy\n",
      "stanza\n",
      "generic\n",
      "tfhub\n",
      "anime_segmentation\n",
      "Doc-UFCN\n",
      "txtai\n",
      "yolov5\n",
      "paddlenlp\n",
      "TTS\n",
      "gpt-neox\n",
      "span-marker\n",
      "tdc\n",
      "tensorflowtts\n",
      "allennlp\n",
      "audiocraft\n",
      "monai\n",
      "fairseq2\n",
      "wildlife-datasets\n",
      "hezar\n",
      "sklearn\n",
      "setfit\n",
      "clmbr\n",
      "mlc-llm\n",
      "RAGatouille\n",
      "unity-sentis\n",
      "Bunkatopics\n",
      "trl\n",
      "LLaVA\n",
      "metavoice\n",
      "llama.cpp\n",
      "gemma_torch\n",
      "whisperkit\n",
      "audioseal\n",
      "grok\n",
      "ml-4m\n",
      "xtuner\n",
      "big_vision\n",
      "GGUF\n",
      "yolov10\n",
      "relik\n",
      "mars5-tts\n",
      "Megatron-LM\n",
      "depth-anything-v2\n",
      "mobileclip\n",
      "light-embed\n",
      "dust3r\n",
      "huatuogpt_vision\n",
      "diambra\n",
      "saelens\n",
      "deepforest\n",
      "diffusionkit\n",
      "refiners\n",
      "opennmt\n",
      "htrflow\n",
      "insightface\n",
      "transformers_zamba2\n",
      "crossformer\n",
      "sapiens\n",
      "cartesia_pytorch\n",
      "ssr-speech\n",
      "llama-omni\n",
      "multimolecule\n",
      "DepthCrafter\n",
      "LogClassifier\n",
      "flux\n",
      "imstoucan\n",
      "sft\n",
      "depth-pro\n",
      "unsloth\n",
      "DepthAnyVideo\n",
      "clipscope\n",
      "allophant\n",
      "anemoi\n",
      "DVT\n",
      "NeMo\n",
      "olmo\n",
      "cxr-foundation\n",
      "derm-foundation\n",
      "terratorch\n",
      "tensorflow\n",
      "segmentation-models-pytorch\n",
      "fastai\n",
      "f5_tts\n",
      "biomed-multi-alignment\n",
      "metamotivo\n",
      "custom\n",
      "safetensors\n",
      "megatron-lm\n",
      "tokenizers\n",
      "tabpfn\n",
      "PyLate\n",
      "gfpgan\n",
      "goodfire-llama-3.3-70b-instruct-sae-l50\n",
      "goodfire-llama-3.1-8b-instruct-sae-l19\n",
      "ollama\n",
      "project-lighter\n",
      "x-mobility\n",
      "staticvectors\n",
      "asteroid\n",
      "k2\n",
      "superb\n",
      "mindspore\n",
      "pytorch-ie\n",
      "fastpitch\n",
      "pytorch-lightning\n",
      "PyTorch Lightning\n",
      "sample-factory\n",
      "k2-sherpa\n",
      "JoeyNMT\n",
      "FastAI\n",
      "pythae\n",
      "mlconsole\n",
      "cleanrl\n",
      "fastMONAI\n",
      "EveryDream\n",
      "kenlm\n",
      "BERT\n",
      "paddle_nlp\n",
      "rl-algo-impls\n",
      "PyLaia\n",
      "lucidrains/gated-state-spaces-pytorch\n",
      "onnx\n",
      "yolov6detect\n",
      "skrl\n",
      "yolor\n",
      "doctr\n",
      "xpmir\n",
      "ppdiffusers\n",
      "mbrl-lib\n",
      "aster\n",
      "archai\n",
      "deep-rl-course\n",
      "Spacy\n",
      "ggml\n",
      "scvi-tools\n",
      "pcdet\n",
      "KerasCV Stable Diffusion in Diffusers\n",
      "tf\n",
      "PaddleNLP\n",
      "Transformers\n",
      "parking-env\n",
      "zeroshot_classifier\n",
      "ctransformers\n",
      "KerasNLP\n",
      "yolo\n",
      "safe-rlhf\n",
      "torch\n",
      "minetest-baselines\n",
      "Core ML\n",
      "mlc\n",
      "Pytorch\n",
      "output-small\n",
      "cotracker\n",
      "braindecode\n",
      "optimum_neuronx\n",
      "yasep\n",
      "bitsandbytes, transformers, peft, accelerate, bitsandbytes, datasets, deepspeed, trl\n",
      "xmen\n",
      "CTranslate2\n",
      "muzero-general\n",
      "pufferlib\n",
      "pytorch_geometric\n",
      "textgen\n",
      "peft - PEFT 0.5.0\n",
      "DanishFungi\n",
      "asteroid1111111111111111111111111111111\n",
      "pruna-engine\n",
      "femr\n",
      "https://github.com/monetjoe/pianos\n",
      "fire-and-smoke\n",
      "TraceBERT\n",
      "clinicadl\n",
      "ExLlamaV2\n",
      "aim\n",
      "transformers, Unsloth, Peft, trl, accelerate, bitsandbytes\n",
      "Flax\n",
      "ml-aim\n",
      "hierarchy-transformers\n",
      "Keras\n",
      "peft,sfttrainer\n",
      "nanotron\n",
      "transformers, peft, torch\n",
      "OpenVINO\n",
      "transformers, peft\n",
      "fasttext, bert\n",
      "tensorflow, keras\n",
      "https://github.com/ICE-PIXIU\n",
      "colbert-ai\n",
      "sglang\n",
      "non\n",
      "mlx-image\n",
      "gemma.cpp\n",
      "keras-hub\n",
      "jax\n",
      "atommic\n",
      "transformers(OpenNMT)\n",
      "v-jepa\n",
      "Transformers PHP\n",
      "whisper\n",
      "clot\n",
      "transformers, pe\n",
      "keras3\n",
      "openphonemizer\n",
      "UniFormer\n",
      "fla\n",
      "UniFormerV2\n",
      "UniDepth\n",
      "voicecraft\n",
      "michelangelo\n",
      "openlm\n",
      "recurrentgemma\n",
      "ditto\n",
      "PyTorchModelHubMixin-template\n",
      "Haystack\n",
      "elm\n",
      "litgpt, transformers\n",
      "rage\n",
      "rasa\n",
      "en-tts\n",
      "my-meta-test\n",
      "Gen AI\n",
      "zho-tts\n",
      "transformers accelerate bitsandbytes\n",
      "pyannote\n",
      "llama-cpp\n",
      "torchvision\n",
      "torchtune\n",
      "keras-nlp\n",
      "llamafile\n",
      "transformers, PyTorch\n",
      "edsnlp\n",
      "ultralyticsplus\n",
      "gptq\n",
      "detectron2\n",
      "delphi\n",
      "burial_mounds\n",
      "fugent\n",
      "prismcaptioner\n",
      "Nvidia Nemo\n",
      "executorch\n",
      "FungiTastic\n",
      "mesh-anything\n",
      "tic-clip\n",
      "DiffusionKit\n",
      "xgboost\n",
      "masa\n",
      "exllamav2\n",
      "bm25s\n",
      "theremin-midi\n",
      "transformers, torch, trl\n",
      "transformer\n",
      "Depth-Anything-V2\n",
      "PyTorch, Transformers\n",
      "configilm\n",
      "Llama-3-8B-peft-QA-QLoRa\n",
      "gemma2.java\n",
      "sklearn and sentence transformers\n",
      "custom_efficientnet\n",
      "TensorFlow Keras\n",
      "open_lm\n",
      "adapters\n",
      "mast3r\n",
      "seed-story\n",
      "PyChemAuth\n",
      "simkgc-kgx\n",
      "nuclia-eval\n",
      "mamba-ssm\n",
      "uce-650m\n",
      "uce-100m\n",
      "gensim\n",
      "co-tracker\n",
      "XGBoost\n",
      "openvino\n",
      "Llama3-8b-FlyingManual-Tutor\n",
      "diffree\n",
      "py-feat\n",
      "BiRefNet\n",
      "GaNDLF\n",
      "distily\n",
      "3dtopia-xl\n",
      "YOLO\n",
      "Distily\n",
      "optimum\n",
      "annif\n",
      "torch, transformers\n",
      "mlxformers\n",
      "mlsae\n",
      "axolotl\n",
      "TensorFlow\n",
      "scaling\n",
      "lstm\n",
      "yolov3\n",
      "cartesia_mlx\n",
      "scikit-learn\n",
      "tonicnet\n",
      "finebooru\n",
      "pxia\n",
      "lofi-bytes\n",
      "hivex\n",
      "FungiTastic Dataset\n",
      "vfi-mamba\n",
      "styletts2\n",
      "exl2\n",
      "transformers, transformers.js\n",
      "ddssd\n",
      "icefall\n",
      "soloaudio\n",
      "ControlNet Preprocessors Annotators\n",
      "quantile-forest\n",
      "sidlingvo\n",
      "moshi_mlx\n",
      "teed\n",
      "yi.java\n",
      "predacons\n",
      "Onnx\n",
      "llama-cpp-python\n",
      "mitie\n",
      "LoRA\n",
      "autrainer\n",
      "gymnasium\n",
      "mantis\n",
      "mergekit\n",
      "nanoGPT\n",
      "speechbrain, ipython, datasets, noisereduce, soundfile, os, torchaudio, torch, transformers\n",
      "Transformers peft  trl\n",
      "prompt-templates\n",
      "turftopic\n",
      "peft 0.13.2\n",
      "bitsandbytes\n",
      "titanic-2410\n",
      "SmallMoleculeMultiView\n",
      "titanic-xgboost-2410\n",
      "transformers, alignment-handbook\n",
      "trtllm\n",
      "hertz-dev\n",
      "fast-geco\n",
      "kaeru-tiny-mlx-upscaler\n",
      "craftsman-v1-5\n",
      "zim-anything\n",
      "diffmeo\n",
      "preferences\n",
      "xxx\n",
      "imscore\n",
      "faster_whisper\n",
      "fasttext, flair\n",
      "Exllamav2\n",
      "spec2psm\n",
      "coremltools\n",
      "sfst\n",
      "mlflow\n",
      "AnyModal\n",
      "birder\n",
      "LLaMA-3.1-8B-AGNews-SFT\n",
      "transformers, deltazip\n",
      "align3r\n",
      "gpt-base-512-clmbr\n",
      "gpt-base-1024-clmbr\n",
      "gpt-base-2048-clmbr\n",
      "gpt-base-4096-clmbr\n",
      "mamba-tiny-1024-clmbr\n",
      "mamba-tiny-4096-clmbr\n",
      "mamba-tiny-8192-clmbr\n",
      "mamba-tiny-16384-clmbr\n",
      "llama-base-512-clmbr\n",
      "llama-base-1024-clmbr\n",
      "llama-base-2048-clmbr\n",
      "llama-base-4096-clmbr\n",
      "hyena-large-1024-clmbr\n",
      "hyena-large-4096-clmbr\n",
      "hyena-large-8192-clmbr\n",
      "hyena-large-16384-clmbr\n",
      "statsmodels\n",
      "transformers,torch,datasers\n",
      "llama_cpp\n",
      "3dImage\n",
      "CoreMLPipelines\n",
      "das3r\n",
      "gradio\n",
      "burn\n",
      "ellipse-rcnn\n",
      "bayesianflow_for_chem\n",
      "popV\n",
      "MAT\n",
      "yolov8\n",
      "rpl\n",
      "prama\n",
      "deepseek-mla\n"
     ]
    }
   ],
   "source": [
    "# library_name 컬럼에서 중복을 제거한 라이브러리 리스트 추출\n",
    "unique_libraries = df['library_name'].dropna().unique()\n",
    "\n",
    "# 라이브러리 이름 출력\n",
    "print(\"Unique libraries in the dataset:\")\n",
    "for library in unique_libraries:\n",
    "    print(library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a63414fa-238a-428d-8e34-8833f0628cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1407537 entries and saved to Model_2025-02-11_output.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "file_path = 'Model_2025-02-11.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 주어진 라이브러리 리스트들\n",
    "libraries_list1 = [\n",
    "    \"transformers\", \"sentence-transformers\", \"keras\", \"adapter-transformers\", \"speechbrain\",\n",
    "    \"espnet\", \"spacy\", \"fasttext\", \"asteroid\", \"generic\", \"ml-agents\", \"stable-baselines3\",\n",
    "    \"pytorch\", \"flair\", \"allennlp\", \"fairseq\", \"k2\", \"tensorflowtts\", \"fastai\", \"opennmt\",\n",
    "    \"timm\", \"sklearn\", \"paddlenlp\", \"superb\", \"doctr\", \"mindspore\", \"pytorch-ie\", \"pyannote-audio\",\n",
    "    \"pysentimiento\", \"stanza\", \"PyTorch\", \"nemo\", \"txtai\", \"fastpitch\", \"pytorch-lightning\",\n",
    "    \"PyTorch Lightning\", \"diffusers\", \"sample-factory\", \"k2-sherpa\", \"JoeyNMT\", \"FastAI\",\n",
    "    \"stable-diffusion\", \"tfhub\", \"pythae\", \"open_clip\", \"mlconsole\", \"cleanrl\", \"setfit\",\n",
    "    \"span-marker\", \"Doc-UFCN\", \"fastMONAI\", \"EveryDream\", \"kenlm\", \"BERT\", \"paddle_nlp\",\n",
    "    \"rl-algo-impls\", \"PyLaia\", \"lucidrains/gated-state-spaces-pytorch\", \"TTS\", \"yolov5\",\n",
    "    \"onnx\", \"yolov6detect\", \"ultralytics\", \"skrl\", \"yolor\", \"xpmir\", \"tensorflow\", \"peft\",\n",
    "    \"scvi-tools\", \"hezar\", \"ppdiffusers\", \"mbrl-lib\", \"aster\", \"gpt-neox\", \"archai\", \"deep-rl-course\",\n",
    "    \"Spacy\", \"ggml\", \"ctranslate2\", \"pcdet\", \"KerasCV Stable Diffusion in Diffusers\", \"tf\",\n",
    "    \"tdc\", \"transformers.js\", \"PaddleNLP\", \"Transformers\", \"parking-env\", \"bertopic\", \"zeroshot_classifier\",\n",
    "    \"ctransformers\", \"KerasNLP\", \"yolo\", \"safetensors\", \"safe-rlhf\", \"torch\", \"minetest-baselines\",\n",
    "    \"audiocraft\", \"Core ML\", \"easydel\", \"wildlife-datasets\", \"monai\", \"fairseq2\", \"mlc\", \"Pytorch\",\n",
    "    \"output-small\", \"braindecode\", \"coqui\", \"optimum_neuronx\", \"yasep\", \"bitsandbytes, transformers, peft, accelerate, bitsandbytes, datasets, deepspeed, trl\",\n",
    "    \"htrflow_core\", \"xmen\", \"CTranslate2\", \"muzero-general\", \"pufferlib\", \"pytorch_geometric\",\n",
    "    \"scikit-learn\", \"textgen\", \"peft - PEFT 0.5.0\", \"mlx\", \"DanishFungi\", \"asteroid1111111111111111111111111111111\",\n",
    "    \"pruna-engine\", \"llama.cpp\", \"seamless_communication\", \"Spacy Explosion\", \"femr\", \"https://github.com/monet-joe/Piano-Classification\",\n",
    "    \"clmbr\", \"xtuner\", \"fire-and-smoke\", \"TraceBERT\", \"Megatron-LM\", \"mlx-llm\", \"RAGatouille\",\n",
    "    \"clinicadl\", \"unity-sentis\", \"ExLlamaV2\", \"tokenizers\", \"aim\", \"Bunkatopics\",\n",
    "    \"transformers, Unsloth, Peft, trl, accelerate, bitsandbytes\", \"Flax\", \"ml-aim\", \"hierarchy-transformers\",\n",
    "    \"Keras\", \"peft,sfttrainer\", \"whisperkit\", \"nanotron\", \"trl\", \"LLaVA\", \"transformers, peft, torch\",\n",
    "    \"OpenVINO\", \"transformers, peft\", \"fasttext, bert\", \"tensorflow, keras\", \"metavoice\", \"gguf\",\n",
    "    \"https://github.com/ICE-PIXIU\", \"colbert-ai\", \"gliner\", \"sglang\", \"non\", \"mlx-image\",\n",
    "    \"GGUF\", \"gemma.cpp\", \"gemma_torch\", \"keras-nlp\", \"jax\", \"atommic\", \"mobileclip\",\n",
    "    \"transformers(OpenNMT)\", \"v-jepa\", \"Transformers PHP\", \"whisper\", \"grok\", \"clot\",\n",
    "    \"transformers, pe\", \"keras3\", \"UniFormer\", \"ml-4m\", \"fla\", \"UniFormerV2\",\n",
    "    \"UniDepth\", \"voicecraft\", \"michelangelo\", \"openlm\", \"recurrentgemma\", \"tflite\",\n",
    "    \"ditto\", \"PyTorchModelHubMixin-template\", \"Haystack\", \"elm\", \"litgpt, transformers\",\n",
    "    \"dust3r\", \"rasa\", \"BiRefNet\", \"en-tts\", \"my-meta-test\", \"Gen AI\", \"zho-tts\",\n",
    "    \"transformers accelerate bitsandbytes\", \"pyannote\", \"llama-cpp\", \"torchvision\",\n",
    "    \"timesfm\", \"torchtune\", \"big_vision\", \"llamafile\", \"light-embed\", \"transformers, PyTorch\",\n",
    "    \"edsnlp\", \"ultralyticsplus\", \"unsloth\", \"gptq\", \"chat_tts\", \"delphi\", \"burial_mounds\",\n",
    "    \"fugent\", \"Nvidia Nemo\", \"segmentation-models-pytorch\", \"executorch\", \"multimolecule\"\n",
    "]\n",
    "\n",
    "libraries_list2 = [\n",
    "    \"PyTorch\", \"TensorFlow\", \"JAX\", \"Transformers\", \"Safetensors\", \"TensorBoard\", \"PEFT\",\n",
    "    \"Diffusers\", \"GGUF\", \"stable-baselines3\", \"ONNX\", \"ml-agents\", \"sentence-transformers\",\n",
    "    \"Keras\", \"Adapters\", \"setfit\", \"timm\", \"sample-factory\", \"Flair\", \"Transformers.js\",\n",
    "    \"MLX\", \"spaCy\", \"fastai\", \"ESPnet\", \"Core ML\", \"OpenVINO\", \"NeMo\", \"Joblib\", \"Rust\",\n",
    "    \"BERTopic\", \"TF Lite\", \"fastText\", \"OpenCLIP\", \"Scikit-learn\", \"PaddlePaddle\", \"speechbrain\",\n",
    "    \"Fairseq\", \"Graphcore\", \"Asteroid\", \"AllenNLP\", \"Stanza\", \"llamafile\", \"SpanMarker\",\n",
    "    \"paddlenlp\", \"Habana\", \"pyannote.audio\", \"pythae\", \"unity-sentis\"\n",
    "]\n",
    "\n",
    "# 첫 글자가 대문자인 경우와 소문자인 경우를 모두 포함한 라이브러리 리스트 생성\n",
    "def create_case_insensitive_list(libraries):\n",
    "    case_insensitive_list = set()\n",
    "    for lib in libraries:\n",
    "        case_insensitive_list.add(lib.lower())\n",
    "        case_insensitive_list.add(lib.capitalize())\n",
    "    return case_insensitive_list\n",
    "\n",
    "normalized_libraries_list1 = set(lib.lower().replace('-', ' ') for lib in libraries_list1)\n",
    "normalized_libraries_list2 = create_case_insensitive_list(libraries_list2)\n",
    "\n",
    "# 새로운 컬럼 생성\n",
    "df['library'] = ''\n",
    "\n",
    "# tags 컬럼을 처리하여 새로운 library 컬럼에 값 넣기\n",
    "for index, row in df.iterrows():\n",
    "    tags = ast.literal_eval(row['tags'])\n",
    "    libraries = [tag for tag in tags if tag.lower().replace('-', ' ') in normalized_libraries_list1 or tag.lower().replace('-', ' ') in normalized_libraries_list2]\n",
    "    df.at[index, 'library'] = ', '.join(libraries)\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "output_csv_file = 'Model_2025-02-11_output.csv'\n",
    "df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(f\"Processed {len(df)} entries and saved to {output_csv_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d0f045a-4ba5-4253-9744-25422b25089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# CSV 파일을 데이터프레임으로 불러오기\n",
    "csv_file_path = 'Model_2025-02-11_output.csv'  # 실제 CSV 파일 경로로 변경하세요\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# 각 라이브러리의 비율 계산\n",
    "library_counts = df['library'].str.split(', ').explode().value_counts(normalize=True)\n",
    "\n",
    "# 정규화 함수\n",
    "def normalize(series):\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "\n",
    "# 로그 변환 및 정규화된 좋아요 수와 다운로드 수 계산\n",
    "df['log_likes'] = np.log1p(df['likes'])  # 로그 변환 적용\n",
    "df['normalized_log_likes'] = normalize(df['log_likes'])\n",
    "df['log_downloads'] = np.log1p(df['downloads'])  # 로그 변환 적용\n",
    "df['normalized_log_downloads'] = normalize(df['log_downloads'])\n",
    "\n",
    "# 업데이트 주기 계산 함수\n",
    "def calculate_update_frequency(created_at, last_modified):\n",
    "    if pd.isna(created_at) or pd.isna(last_modified):\n",
    "        return 0\n",
    "    created_date = datetime.strptime(created_at, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "    last_modified_date = datetime.strptime(last_modified, \"%Y-%m-%d %H:%M:%S%z\")\n",
    "    days_since_creation = (last_modified_date - created_date).days\n",
    "    return days_since_creation\n",
    "\n",
    "# 업데이트 주기 계산 및 1년 내 업데이트 여부 체크\n",
    "df['update_frequency'] = df.apply(lambda row: calculate_update_frequency(row['created_at'], row['last_modified']), axis=1)\n",
    "df['last_modified_date'] = pd.to_datetime(df['last_modified'], format=\"%Y-%m-%d %H:%M:%S%z\")\n",
    "one_year_ago = datetime.now(datetime.now().astimezone().tzinfo) - timedelta(days=365)\n",
    "df['within_one_year'] = df['last_modified_date'] > one_year_ago\n",
    "\n",
    "# 1년 내 업데이트된 데이터만을 사용하여 정규화 진행\n",
    "df_within_one_year = df[df['within_one_year']].copy()\n",
    "df_within_one_year['normalized_update_frequency'] = normalize(df_within_one_year['update_frequency'])\n",
    "\n",
    "# 정규화된 업데이트 주기 병합\n",
    "df = df.merge(df_within_one_year[['id', 'normalized_update_frequency']], on='id', how='left')\n",
    "df['normalized_update_frequency'] = df['normalized_update_frequency'].fillna(0)\n",
    "\n",
    "# 기준 점수 설정\n",
    "BASE_LOG_LIKES_SCORE = 30\n",
    "BASE_UPDATE_FREQUENCY_SCORE = 10\n",
    "BASE_LOG_DOWNLOADS_SCORE = 50\n",
    "BASE_LIBRARY_SCORE = 10\n",
    "AUTO_GATED_PENALTY = -10\n",
    "MANUAL_GATED_PENALTY = -20\n",
    "\n",
    "# 영향력 점수 계산 함수\n",
    "def calculate_influence_score(row):\n",
    "    scores = {}\n",
    "    \n",
    "    # 좋아요 수 기반 점수 계산 (정규화된 로그 좋아요 수 * 30점)\n",
    "    log_likes_score = row['normalized_log_likes'] * BASE_LOG_LIKES_SCORE\n",
    "    scores['log_likes_score'] = log_likes_score\n",
    "    \n",
    "    # 업데이트 주기 기반 점수 계산 (정규화된 업데이트 주기 * 10점)\n",
    "    update_frequency_score = row['normalized_update_frequency'] * BASE_UPDATE_FREQUENCY_SCORE\n",
    "    scores['update_frequency_score'] = update_frequency_score\n",
    "    \n",
    "    # 최근 30일 간 다운로드 수 기반 점수 계산 (정규화된 로그 다운로드 수 * 50점)\n",
    "    log_downloads_score = row['normalized_log_downloads'] * BASE_LOG_DOWNLOADS_SCORE\n",
    "    scores['log_downloads_score'] = log_downloads_score\n",
    "    \n",
    "    # 라이브러리 이름 기반 점수 계산 (비율 기반 가중치 적용, 최대 10점)\n",
    "    library_names = row['library_name'].split(', ') if pd.notna(row['library_name']) else []\n",
    "    library_weight = sum(library_counts.get(lib, 0) * BASE_LIBRARY_SCORE for lib in library_names)\n",
    "    scores['library_weight'] = library_weight\n",
    "\n",
    "    # 게이트 처리 여부 점수 조정\n",
    "    if row['gated'] == 'auto':\n",
    "        gated_penalty = AUTO_GATED_PENALTY\n",
    "    elif row['gated'] == 'manual':\n",
    "        gated_penalty = MANUAL_GATED_PENALTY\n",
    "    else:\n",
    "        gated_penalty = 0\n",
    "    scores['gated_penalty'] = gated_penalty\n",
    "    \n",
    "    # 총합 점수 계산\n",
    "    total_score = log_likes_score + update_frequency_score + log_downloads_score + scores['library_weight'] + gated_penalty\n",
    "    scores['total_score'] = total_score\n",
    "    \n",
    "    return pd.Series(scores)\n",
    "\n",
    "# 새로운 데이터프레임 생성\n",
    "scores_df = df.apply(calculate_influence_score, axis=1)\n",
    "result_df = df[['id', 'likes', 'created_at', 'last_modified', 'downloads', 'library_name', 'gated']].join(scores_df)\n",
    "\n",
    "# 인덱스 초기화\n",
    "result_df = result_df.reset_index(drop=True)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "result_df.to_csv('Model_2025-02-11_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf8b06bb-fec0-4b77-be98-082b7931297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 열기\n",
    "file_path = \"Model_2025-02-11_output.csv\"  # 실제 파일 경로로 변경하세요\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# 총 다운로드 수와 좋아요 수 계산\n",
    "total_downloads = data['downloads'].sum()\n",
    "total_likes = data['likes'].sum()\n",
    "\n",
    "# 다운로드 수와 좋아요 수의 비율 계산 (총 다운로드 수 / 총 좋아요 수)\n",
    "ratio = total_downloads / total_likes  # 예: 1980.65\n",
    "\n",
    "# 다운로드 점수를 5/3로 조정\n",
    "data['download_score'] = (data['downloads'] / ratio) * (5 / 3)\n",
    "data['like_score'] = data['likes'] * 1\n",
    "\n",
    "# 최종 점수 계산: 다운로드 수 점수 + 좋아요 수 점수\n",
    "data['total_score'] = data['download_score'] + data['like_score']\n",
    "\n",
    "# 정렬된 데이터 출력 (정렬 없이 저장)\n",
    "data.to_csv(\"Model_2025-02-11_influence.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "867e5d13-f05d-4b71-a762-ce4cf85f7ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kodonghwan\\AppData\\Local\\Temp\\ipykernel_16808\\2404542778.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected.rename(columns=columns_to_rename, inplace=True)\n",
      "C:\\Users\\kodonghwan\\AppData\\Local\\Temp\\ipykernel_16808\\2404542778.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected.rename(columns=columns_to_rename, inplace=True)\n",
      "C:\\Users\\kodonghwan\\AppData\\Local\\Temp\\ipykernel_16808\\2404542778.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected.rename(columns=columns_to_rename, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 CSV 파일로 저장되었습니다: 6-11_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 각 CSV 파일의 경로\n",
    "file_paths = [\n",
    "    'Model_2025-02-11_output.csv', \n",
    "    'Model_2025-02-11_score.csv', \n",
    "    'Model_2025-02-11_influence.csv'\n",
    "]\n",
    "\n",
    "# 각 파일에서 필요한 컬럼 이름과 새로운 이름 (일부만 변경)\n",
    "columns_to_extract_and_rename = {\n",
    "    'Model_2025-02-11_output.csv': {\n",
    "        'id': 'id',\n",
    "        'author': 'author',\n",
    "        'created_at': 'created_at',\n",
    "        'last_modified': 'last_modified',\n",
    "        'private': 'private',\n",
    "        'gated': 'gated',\n",
    "        'disabled': 'disabled',\n",
    "        'downloads': 'downloads',\n",
    "        'likes': 'likes',\n",
    "        'library_name': 'library_name',\n",
    "        'library': 'all_library',\n",
    "        'tags': 'tags',\n",
    "        'pipeline_tag': 'pipeline_tag',\n",
    "        'arxiv': 'arxiv',\n",
    "        'dataset': 'dataset',\n",
    "        'license': 'license'\n",
    "    },\n",
    "    'Model_2025-02-11_score.csv': {\n",
    "        'total_score': 'model_score'\n",
    "    },\n",
    "    'Model_2025-02-11_influence.csv': {\n",
    "        'total_score': 'influence_score'\n",
    "    }\n",
    "}\n",
    "\n",
    "# 빈 데이터프레임 생성\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# 각 파일을 열고 필요한 컬럼만 추출 및 이름 변경하여 합치기\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    columns_to_rename = columns_to_extract_and_rename[file_path]\n",
    "    df_selected = df[list(columns_to_rename.keys())]\n",
    "    df_selected.rename(columns=columns_to_rename, inplace=True)\n",
    "    combined_df = pd.concat([combined_df, df_selected], axis=1)\n",
    "\n",
    "# 새로운 CSV 파일로 저장\n",
    "combined_df.to_csv('Model_2025-02-11_final.csv', index=False)\n",
    "\n",
    "print(\"새로운 CSV 파일로 저장되었습니다: 6-11_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43f77489-f713-4cf0-80cc-d4b131cc5485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         id       author  \\\n",
      "0                   deepseek-ai/DeepSeek-R1  deepseek-ai   \n",
      "1  ValueFX9507/Tifa-Deepsex-14b-CoT-GGUF-Q4  ValueFX9507   \n",
      "2                  deepseek-ai/Janus-Pro-7B  deepseek-ai   \n",
      "3                  Zyphra/Zonos-v0.1-hybrid       Zyphra   \n",
      "4                        hexgrad/Kokoro-82M      hexgrad   \n",
      "\n",
      "                  created_at              last_modified  private  gated  \\\n",
      "0  2025-01-20 03:46:07+00:00  2025-02-09 06:34:11+00:00    False  False   \n",
      "1  2025-02-04 07:20:55+00:00  2025-02-08 09:58:51+00:00    False  False   \n",
      "2  2025-01-26 12:05:50+00:00  2025-02-01 08:00:16+00:00    False  False   \n",
      "3  2025-02-06 22:35:07+00:00  2025-02-10 18:44:11+00:00    False  False   \n",
      "4  2024-12-26 00:20:08+00:00  2025-02-01 23:54:42+00:00    False  False   \n",
      "\n",
      "   disabled  downloads  likes  library_name                all_library  \\\n",
      "0       NaN    2670920   8176  transformers  transformers, safetensors   \n",
      "1       NaN      40272    409  transformers         transformers, gguf   \n",
      "2       NaN     369829   2838  transformers      transformers, pytorch   \n",
      "3       NaN         80    320           NaN                safetensors   \n",
      "4       NaN     300454   3019           NaN                        NaN   \n",
      "\n",
      "                                                tags            pipeline_tag  \\\n",
      "0  ['transformers', 'safetensors', 'deepseek_v3',...         text-generation   \n",
      "1  ['transformers', 'gguf', 'incremental-pretrain...  reinforcement-learning   \n",
      "2  ['transformers', 'pytorch', 'multi_modality', ...              any-to-any   \n",
      "3                  ['safetensors', 'text-to-speech']          text-to-speech   \n",
      "4  ['text-to-speech', 'en', 'base_model:yl4579/St...          text-to-speech   \n",
      "\n",
      "              arxiv dataset             license  model_score  influence_score  \n",
      "0  arxiv:2501.12948     NaN         license:mit    72.962647     10943.336166  \n",
      "1               NaN     NaN  license:apache-2.0    51.981658       450.725758  \n",
      "2  arxiv:2501.17811     NaN         license:mit    64.185230      3221.179267  \n",
      "3               NaN     NaN  license:apache-2.0    32.537650       320.082888  \n",
      "4  arxiv:2203.02395     NaN  license:apache-2.0    61.674714      3330.299934  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "file_path = 'Model_2025-02-11_final.csv'  # 파일 경로를 업데이트하세요\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 'influence_score' 컬럼을 기준으로 데이터프레임 정렬\n",
    "df_sorted = df.sort_values(by='influence_score', ascending=False)\n",
    "\n",
    "# 상위 10000개의 행 추출\n",
    "df_top_10000 = df_sorted.head(10000)\n",
    "\n",
    "# 추출된 데이터프레임을 인덱스 순서대로 다시 정렬\n",
    "df_top_10000_sorted_by_index = df_top_10000.sort_index()\n",
    "\n",
    "# 결과 데이터프레임 출력\n",
    "print(df_top_10000_sorted_by_index.head())\n",
    "\n",
    "# 결과 데이터프레임을 새로운 CSV 파일로 저장 (선택사항)\n",
    "df_top_10000_sorted_by_index.to_csv('Model_2025-02-11_influence_top_10000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd773d36-252a-48dc-a944-f060120e6ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|████████████████████████████████████████████████████████████████| 25/25 [01:23<00:00,  3.35s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. top_10000_influence_score.csv을 열어 df에 저장\n",
    "df = pd.read_csv('6-11_influence_top_10000.csv')\n",
    "\n",
    "# 2. df의 id 컬럼의 데이터를 가져옴\n",
    "ids = df['id']\n",
    "\n",
    "# 3. df_downloads, df_likes 데이터프레임 생성\n",
    "df_downloads = pd.DataFrame(ids)\n",
    "df_likes = pd.DataFrame(ids)\n",
    "\n",
    "# 4. C:\\Users\\han\\모델 메타데이터에 있는 모든 csv 파일을 순차적으로 염\n",
    "directory = 'C:\\\\Users\\\\han\\\\모델 메타데이터'\n",
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "\n",
    "# 반복 작업\n",
    "for file in tqdm(csv_files, desc='Processing files'):\n",
    "    file_path = os.path.join(directory, file)\n",
    "    temp_df = pd.read_csv(file_path)\n",
    "\n",
    "    # 5. 파일의 이름을 제목으로 df_downloads, df_likes에 새로운 컬럼으로 추가\n",
    "    column_name = os.path.splitext(file)[0]\n",
    "    df_downloads[column_name] = 0\n",
    "    df_likes[column_name] = 0\n",
    "\n",
    "    # 6. df의 id 컬럼과 순차적으로 연 파일의 id 컬럼을 비교하여 존재하는 id의 행만 추출\n",
    "    merged_df = pd.merge(df[['id']], temp_df[['id', 'downloads', 'likes']], on='id', how='inner')\n",
    "\n",
    "    # 7. 추출한 행의 downloads와 likes를 각 파일 이름과 동일한 컬럼에 입력\n",
    "    df_downloads.loc[df_downloads['id'].isin(merged_df['id']), column_name] = merged_df['downloads'].values\n",
    "    df_likes.loc[df_likes['id'].isin(merged_df['id']), column_name] = merged_df['likes'].values\n",
    "\n",
    "# 8. df_downloads와 df_likes를 각각 csv 파일로 저장\n",
    "df_downloads.to_csv('df_downloads.csv', index=False)\n",
    "df_likes.to_csv('df_likes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9696a2e6-2ca8-44f8-aeb6-a8fddf431ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일이 likes_changes.csv에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 로드\n",
    "file_path = 'df_likes.csv'  # 실제 파일 경로로 변경해주세요\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 첫 번째 컬럼(id)를 제외한 모든 컬럼 선택\n",
    "df_likes = df.iloc[:, 1:]\n",
    "\n",
    "# 좋아요 수 증감량 계산\n",
    "likes_diff = df_likes.diff(axis=1)\n",
    "\n",
    "# 좋아요 수 증감률 계산\n",
    "likes_diff_percent = df_likes.pct_change(axis=1) * 100\n",
    "\n",
    "# 결과를 하나의 DataFrame으로 결합\n",
    "result_df = pd.concat([df['id'], likes_diff, likes_diff_percent], axis=1)\n",
    "result_df.columns = ['id'] + [f'{col}_diff' for col in df_likes.columns] + [f'{col}_diff_percent' for col in df_likes.columns]\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "output_file_path = 'likes_changes.csv'  # 실제 파일 경로로 변경해주세요\n",
    "result_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f'CSV 파일이 {output_file_path}에 저장되었습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c935f1d7-6988-4b08-87e3-e9a5d472403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching dataset metadata: 163262it [02:07, 1282.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched and saved to CSV file successfully!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hugging Face API 인스턴스 생성\n",
    "api = HfApi()\n",
    "\n",
    "# 데이터셋 리스트를 가져오기 (데이터셋 메타 데이터 포함)\n",
    "datasets_generator = api.list_datasets(full=True)\n",
    "\n",
    "# 데이터셋 메타 데이터를 저장할 리스트 초기화\n",
    "datasets_data = []\n",
    "\n",
    "# 데이터셋 메타 데이터를 가져오는 동안 진행도 표시\n",
    "for dataset in tqdm(datasets_generator, desc=\"Fetching dataset metadata\"):\n",
    "    datasets_data.append(dataset)\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "df = pd.DataFrame(datasets_data)\n",
    "\n",
    "# 데이터프레임을 CSV 파일로 저장 (선택 사항)\n",
    "df.to_csv('datasets_metadata.csv', index=False)\n",
    "\n",
    "print(\"Data fetched and saved to CSV file successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e4502-6c92-4d99-b2eb-63b0cf43d337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
